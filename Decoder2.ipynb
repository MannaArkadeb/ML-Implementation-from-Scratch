{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d67b856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Tue_May_27_02:24:01_Pacific_Daylight_Time_2025\n",
      "Cuda compilation tools, release 12.9, V12.9.86\n",
      "Build cuda_12.9.r12.9/compiler.36037853_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8945dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cupy-cuda12x in c:\\users\\manna\\anaconda3\\lib\\site-packages (13.5.1)\n",
      "Requirement already satisfied: numpy<2.6,>=1.22 in c:\\users\\manna\\anaconda3\\lib\\site-packages (from cupy-cuda12x) (1.26.4)\n",
      "Requirement already satisfied: fastrlock>=0.5 in c:\\users\\manna\\anaconda3\\lib\\site-packages (from cupy-cuda12x) (0.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install cupy-cuda12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c342db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 23 14:47:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.57                 Driver Version: 576.57         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   59C    P0             16W /   85W |       0MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71aabe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "def dropout(x, rate=0.1, training=True):\n",
    "    if not training or rate == 0:\n",
    "        return x\n",
    "    mask = (cp.random.rand(*x.shape) > rate).astype(cp.float32)\n",
    "    return x * mask / (1.0 - rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb59be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamW:\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = {name: cp.zeros_like(p) for name, p in params}\n",
    "        self.v = {name: cp.zeros_like(p) for name, p in params}\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, grads):\n",
    "        self.t += 1\n",
    "        lr_t = self.lr\n",
    "\n",
    "        for name, param in self.params:\n",
    "            grad = grads[name]\n",
    "            m = self.m[name]\n",
    "            v = self.v[name]\n",
    "            m *= self.beta1\n",
    "            m += (1 - self.beta1) * grad\n",
    "\n",
    "            v *= self.beta2\n",
    "            v += (1 - self.beta2) * (grad * grad) \n",
    "            m_hat = m / (1 - self.beta1 ** self.t)\n",
    "            v_hat = v / (1 - self.beta2 ** self.t)\n",
    "            param *= (1 - lr_t * self.weight_decay)\n",
    "            param -= lr_t * m_hat / (cp.sqrt(v_hat) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b45851",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text):\n",
    "        cleaned = ''.join(c.lower() if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "        words = cleaned.split()\n",
    "\n",
    "        unique_words = sorted(list(set(words)))\n",
    "        unique_words.append(\"<unk>\")\n",
    "\n",
    "        self.word2idx = {w: i for i, w in enumerate(unique_words)}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "        self.vocab_size = len(unique_words)\n",
    "\n",
    "    def encode(self, text):\n",
    "        cleaned = ''.join(c.lower() if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "        words = cleaned.split()\n",
    "        return [self.word2idx.get(w, self.word2idx[\"<unk>\"]) for w in words]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word.get(i, '<unk>') for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f9a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.weights = cp.random.randn(vocab_size, embed_dim) * 0.01\n",
    "        self.grad = cp.zeros_like(self.weights) \n",
    "\n",
    "    def forward(self, indices):\n",
    "        self.last_indices = indices\n",
    "        return self.weights[indices]  \n",
    "\n",
    "    def backward(self, doutput):\n",
    "        self.grad[...] = 0\n",
    "        B, T, D = doutput.shape\n",
    "        flat_indices = self.last_indices.reshape(B * T)  \n",
    "        doutput_flat = doutput.reshape(B * T, D)\n",
    "\n",
    "        for k in range(B * T):\n",
    "            token_id = flat_indices[k]\n",
    "            self.grad[token_id] += doutput_flat[k]\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return [('embedding_weights', self.weights)]\n",
    "\n",
    "    def grads(self):\n",
    "        return {'embedding_weights': self.grad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04640b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        self.encoding = cp.zeros((max_len, embed_dim), dtype=cp.float32)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, embed_dim, 2):\n",
    "                angle = pos / (10000 ** ((2 * i) / embed_dim))\n",
    "                self.encoding[pos, i] = cp.sin(angle)\n",
    "                if i + 1 < embed_dim:\n",
    "                    self.encoding[pos, i + 1] = cp.cos(angle)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 2:\n",
    "            x = x[cp.newaxis, :, :]\n",
    "        B, T, D = x.shape\n",
    "\n",
    "        if T > self.encoding.shape[0]:\n",
    "            raise ValueError(f\"Sequence length ({T}) exceeds max_len used in PositionalEncoding.\")\n",
    "        pos_encoding = self.encoding[:T]  \n",
    "        return x + pos_encoding[cp.newaxis, :, :]  \n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    x = x - cp.max(x, axis=axis, keepdims=True) \n",
    "    exp_x = cp.exp(x)\n",
    "    return exp_x / cp.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "class ScaledDotProductAttention:\n",
    "    def __init__(self, embed_dim):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.scale = cp.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        self.Q = Q \n",
    "        self.K = K\n",
    "        self.V = V\n",
    "\n",
    "        self.scores = cp.matmul(Q, K.transpose(0, 2, 1)) / self.scale  \n",
    "        if mask is not None:\n",
    "            assert mask.ndim == 2\n",
    "            mask = mask[cp.newaxis, :, :]  \n",
    "            self.scores = cp.where(mask == 0, -1e9, self.scores)\n",
    "\n",
    "        self.weights = softmax(self.scores, axis=-1) \n",
    "        self.output = cp.matmul(self.weights, V) \n",
    "        return self.output, self.weights\n",
    "\n",
    "    def backward(self, doutput):\n",
    "      \"\"\"\n",
    "      doutput: (B, T_q, D)\n",
    "      Returns: dQ, dK, dV\n",
    "      \"\"\"\n",
    "      B, T_q, D = doutput.shape\n",
    "      _, T_k, _ = self.V.shape\n",
    "      dW = cp.matmul(doutput, self.V.transpose(0, 2, 1)) \n",
    "      dV = cp.matmul(self.weights.transpose(0, 2, 1), doutput) \n",
    "      d_scores = self._softmax_backward(self.weights, dW)\n",
    "      d_scores /= self.scale\n",
    "      dQ = cp.matmul(d_scores, self.K)                 \n",
    "      dK = cp.matmul(d_scores.transpose(0, 2, 1), self.Q)  \n",
    "      return dQ, dK, dV\n",
    "\n",
    "    def _softmax_backward(self, softmax_output, grad_output):\n",
    "      \"\"\"\n",
    "      Efficient Jacobian-vector product for softmax.\n",
    "      softmax_output: (B, T_q, T_k)\n",
    "      grad_output:    (B, T_q, T_k)\n",
    "      Returns:        (B, T_q, T_k)\n",
    "      \"\"\"\n",
    "      dot = cp.sum(grad_output * softmax_output, axis=-1, keepdims=True) \n",
    "      return softmax_output * (grad_output - dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.W_q = cp.random.randn(embed_dim, embed_dim) * 0.01\n",
    "        self.W_k = cp.random.randn(embed_dim, embed_dim) * 0.01\n",
    "        self.W_v = cp.random.randn(embed_dim, embed_dim) * 0.01\n",
    "        self.W_o = cp.random.randn(embed_dim, embed_dim) * 0.01\n",
    "        self.dW_q = cp.zeros_like(self.W_q)\n",
    "        self.dW_k = cp.zeros_like(self.W_k)\n",
    "        self.dW_v = cp.zeros_like(self.W_v)\n",
    "        self.dW_o = cp.zeros_like(self.W_o)\n",
    "        self.attn = ScaledDotProductAttention(self.head_dim)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, emb_dim = x.shape\n",
    "        assert emb_dim == self.num_heads * self.head_dim, \"emb_dim must be divisible by num_heads\"\n",
    "        return x.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        return x.reshape(x.shape[0], self.embed_dim)\n",
    "\n",
    "    def forward(self, Q_input, K_input, V_input, mask=None):\n",
    "        self.Q_input = Q_input  \n",
    "        self.K_input = K_input\n",
    "        self.V_input = V_input\n",
    "\n",
    "        Q = cp.dot(Q_input, self.W_q)\n",
    "        K = cp.dot(K_input, self.W_k)\n",
    "        V = cp.dot(V_input, self.W_v)\n",
    "\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        self.Q, self.K, self.V = Q, K, V \n",
    "\n",
    "        heads_output = []\n",
    "        self.attn_outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            out, attn = self.attn.forward(Q[:, i], K[:, i], V[:, i], mask)\n",
    "            heads_output.append(out)\n",
    "            self.attn_outputs.append(attn)\n",
    "\n",
    "        concat = cp.concatenate(heads_output, axis=-1)\n",
    "        self.concat = concat  \n",
    "\n",
    "        output = cp.dot(concat, self.W_o)\n",
    "        return output, self.attn_outputs\n",
    "    def backward(self, dout):\n",
    "      B, T, D = dout.shape\n",
    "      H, d = self.num_heads, self.head_dim\n",
    "      dconcat = cp.dot(dout, self.W_o.T)\n",
    "      self.dW_o += cp.dot(self.concat.reshape(-1, D).T, dout.reshape(-1, D))\n",
    "      dheads = dconcat.reshape(B, T, H, d).transpose(0, 2, 1, 3)\n",
    "      dQ = cp.zeros_like(self.Q)\n",
    "      dK = cp.zeros_like(self.K)\n",
    "      dV = cp.zeros_like(self.V)\n",
    "      for h in range(H):\n",
    "          dqh, dkh, dvh = self.attn.backward(dheads[:, h]) \n",
    "          dQ[:, h], dK[:, h], dV[:, h] = dqh, dkh, dvh\n",
    "      dQ_flat = dQ.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
    "      dK_flat = dK.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
    "      dV_flat = dV.transpose(0, 2, 1, 3).reshape(B, T, D)\n",
    "      Q_in = self.Q_input.reshape(-1, D)\n",
    "      K_in = self.K_input.reshape(-1, D)\n",
    "      V_in = self.V_input.reshape(-1, D)\n",
    "\n",
    "      self.dW_q += cp.dot(Q_in.T, dQ_flat.reshape(-1, D))\n",
    "      self.dW_k += cp.dot(K_in.T, dK_flat.reshape(-1, D))\n",
    "      self.dW_v += cp.dot(V_in.T, dV_flat.reshape(-1, D))\n",
    "      dQ_input = cp.dot(dQ_flat, self.W_q.T)\n",
    "      dK_input = cp.dot(dK_flat, self.W_k.T)\n",
    "      dV_input = cp.dot(dV_flat, self.W_v.T)\n",
    "\n",
    "      return dQ_input + dK_input + dV_input\n",
    "\n",
    "    def parameters(self):\n",
    "        return [\n",
    "            (\"W_q\", self.W_q),\n",
    "            (\"W_k\", self.W_k),\n",
    "            (\"W_v\", self.W_v),\n",
    "            (\"W_o\", self.W_o),\n",
    "        ]\n",
    "\n",
    "    def grads(self):\n",
    "        return {\n",
    "            'W_q': self.dW_q,\n",
    "            'W_k': self.dW_k,\n",
    "            'W_v': self.dW_v,\n",
    "            'W_o': self.dW_o\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89244a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        self.W1 = cp.random.randn(embed_dim, hidden_dim) * 0.01\n",
    "        self.b1 = cp.zeros((hidden_dim,))\n",
    "        self.W2 = cp.random.randn(hidden_dim, embed_dim) * 0.01\n",
    "        self.b2 = cp.zeros((embed_dim,))\n",
    "        self.dW1 = cp.zeros_like(self.W1)\n",
    "        self.db1 = cp.zeros_like(self.b1)\n",
    "        self.dW2 = cp.zeros_like(self.W2)\n",
    "        self.db2 = cp.zeros_like(self.b2)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return cp.maximum(0, x)\n",
    "\n",
    "    def relu_grad(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.h1 = cp.matmul(x, self.W1) + self.b1 \n",
    "        self.a1 = self.relu(self.h1)\n",
    "        self.h2 = cp.matmul(self.a1, self.W2) + self.b2 \n",
    "        return self.h2\n",
    "\n",
    "    def backward(self, dout):\n",
    "      B, T, D = dout.shape\n",
    "      dx2 = dout.reshape(-1, D)                         \n",
    "      a1_flat = self.a1.reshape(-1, self.a1.shape[-1])  \n",
    "      self.dW2 += cp.dot(a1_flat.T, dx2)                \n",
    "      self.db2 += cp.sum(dx2, axis=0)\n",
    "      da1 = cp.dot(dx2, self.W2.T).reshape(self.a1.shape)\n",
    "      dh1 = da1 * self.relu_grad(self.h1)\n",
    "      x_flat = self.x.reshape(-1, self.x.shape[-1])      \n",
    "      dh1_flat = dh1.reshape(-1, dh1.shape[-1])        \n",
    "      self.dW1 += cp.dot(x_flat.T, dh1_flat)          \n",
    "      self.db1 += cp.sum(dh1, axis=(0, 1))\n",
    "      dx = cp.dot(dh1, self.W1.T)                       \n",
    "      return dx\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [\n",
    "            (\"W1\", self.W1),\n",
    "            (\"b1\", self.b1),\n",
    "            (\"W2\", self.W2),\n",
    "            (\"b2\", self.b2)\n",
    "        ]\n",
    "\n",
    "    def grads(self):\n",
    "        return {\n",
    "            \"W1\": self.dW1,\n",
    "            \"b1\": self.db1,\n",
    "            \"W2\": self.dW2,\n",
    "            \"b2\": self.db2\n",
    "        }\n",
    "\n",
    "    def param_names(self):\n",
    "        return ['ffn_W1', 'ffn_b1', 'ffn_W2', 'ffn_b2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, embed_dim, eps=1e-5):\n",
    "        self.gamma = cp.ones((embed_dim,))\n",
    "        self.beta = cp.zeros((embed_dim,))\n",
    "        self.eps = eps\n",
    "        self.grad_gamma = cp.zeros_like(self.gamma)\n",
    "        self.grad_beta = cp.zeros_like(self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  \n",
    "        self.mean = cp.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = cp.var(x, axis=-1, keepdims=True)\n",
    "        self.std = cp.sqrt(self.var + self.eps)\n",
    "        self.norm = (x - self.mean) / self.std\n",
    "        return self.gamma * self.norm + self.beta\n",
    "\n",
    "    def backward(self, dout):\n",
    "        self.grad_gamma = cp.sum(dout * self.norm, axis=tuple(range(dout.ndim - 1)))\n",
    "        self.grad_beta = cp.sum(dout, axis=tuple(range(dout.ndim - 1)))\n",
    "\n",
    "        N = self.x.shape[-1]\n",
    "        x_mu = self.x - self.mean\n",
    "        dnorm = dout * self.gamma\n",
    "\n",
    "        dvar = cp.sum(dnorm * x_mu * -0.5 * (self.std**-3), axis=-1, keepdims=True)\n",
    "        dmean = cp.sum(dnorm * -1 / self.std, axis=-1, keepdims=True) + dvar * np.mean(-2. * x_mu, axis=-1, keepdims=True)\n",
    "\n",
    "        dx = (dnorm / self.std) + (dvar * 2 * x_mu / N) + (dmean / N)\n",
    "        return dx\n",
    "\n",
    "    def parameters(self):\n",
    "        return [('layernorm_gamma', self.gamma), ('layernorm_beta', self.beta)]\n",
    "\n",
    "    def grads(self):\n",
    "        return {'layernorm_gamma': self.grad_gamma, 'layernorm_beta': self.grad_beta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06453cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock:\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        self.ln1 = LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ln2 = LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, hidden_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, mask, training=True):\n",
    "        self.x_input = x  \n",
    "        self.x_ln1 = self.ln1.forward(x)\n",
    "        self.attn_out, _ = self.attn.forward(self.x_ln1, self.x_ln1, self.x_ln1, mask)\n",
    "        self.attn_out = dropout(self.attn_out, rate=self.dropout_rate, training=training)\n",
    "        self.x_after_attn = x + self.attn_out \n",
    "        self.x_ln2 = self.ln2.forward(self.x_after_attn)\n",
    "        self.ffn_out = self.ffn.forward(self.x_ln2)\n",
    "        self.ffn_out = dropout(self.ffn_out, rate=self.dropout_rate, training=training)\n",
    "        self.x_out = self.x_after_attn + self.ffn_out \n",
    "        return self.x_out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dffn_out = dout\n",
    "        dx_after_attn = dout \n",
    "        dffn_out = self.ffn.backward(dffn_out)\n",
    "        dln2_out = self.ln2.backward(dffn_out)\n",
    "        dattn_out = dx_after_attn + dln2_out \n",
    "        dattn_out = self.attn.backward(dattn_out)\n",
    "        dln1_out = self.ln1.backward(dattn_out)\n",
    "\n",
    "        return self.x_input + dln1_out\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for k, v in self.ln1.parameters():\n",
    "            params.append((f\"ln1.{k}\", v))\n",
    "        for k, v in self.attn.parameters():\n",
    "            params.append((f\"attn.{k}\", v))\n",
    "        for k, v in self.ln2.parameters():\n",
    "            params.append((f\"ln2.{k}\", v))\n",
    "        for k, v in self.ffn.parameters():\n",
    "            params.append((f\"ffn.{k}\", v))\n",
    "        return params\n",
    "    def grads(self):\n",
    "        grads = {}\n",
    "        for name, grad in self.ln1.grads().items():\n",
    "            grads[f\"ln1.{name}\"] = grad\n",
    "        for name, grad in self.attn.grads().items():\n",
    "            grads[f\"attn.{name}\"] = grad\n",
    "        for name, grad in self.ln2.grads().items():\n",
    "            grads[f\"ln2.{name}\"] = grad\n",
    "        for name, grad in self.ffn.grads().items():\n",
    "            grads[f\"ffn.{name}\"] = grad\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912e9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer:\n",
    "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, hidden_dim, num_layers):\n",
    "        self.embed = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(max_len, embed_dim)\n",
    "        self.blocks = [DecoderBlock(embed_dim, num_heads, hidden_dim, dropout_rate=0.1) for _ in range(num_layers)]\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "        self.output_layer = cp.random.randn(embed_dim, vocab_size) * 0.01\n",
    "        self.output_grad = cp.zeros_like(self.output_layer)\n",
    "\n",
    "    def forward(self, x_indices, training=True):\n",
    "        x = self.embed.forward(x_indices)\n",
    "        x = self.pos_enc.forward(x)\n",
    "        seq_len = x.shape[1]\n",
    "        mask = cp.tril(np.ones((seq_len, seq_len)))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x, mask, training=training)\n",
    "\n",
    "        x = self.ln.forward(x)\n",
    "        self.last_hidden = x \n",
    "        logits = cp.dot(x, self.output_layer)\n",
    "        return logits, x\n",
    "\n",
    "    def backward_output_layer(self, dlogits):\n",
    "        x = self.last_hidden\n",
    "        dW_out = cp.dot(x.reshape(-1, x.shape[-1]).T, dlogits.reshape(-1, dlogits.shape[-1]))\n",
    "        dx = cp.dot(dlogits, self.output_layer.T)\n",
    "\n",
    "        self.output_grad[...] = dW_out\n",
    "        return dx\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        dx = self.backward_output_layer(dlogits)\n",
    "        dx = self.ln.backward(dx)\n",
    "        for block in reversed(self.blocks):\n",
    "            dx = block.backward(dx)\n",
    "        dx = self.pos_enc.backward(dx)\n",
    "        self.embed.backward(dx)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "\n",
    "        params.append((\"output_layer\", self.output_layer))\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            for name, value in block.parameters():\n",
    "                params.append((f\"blocks.{i}.{name}\", value))\n",
    "        for name, value in self.embed.parameters():\n",
    "            params.append((f\"embed.{name}\", value))\n",
    "        return params\n",
    "\n",
    "    def grads(self):\n",
    "        grads = {'output_layer': self.output_grad}\n",
    "\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            block_grads = block.grads()\n",
    "            for name, grad in block_grads.items():\n",
    "                grads[f\"blocks.{i}.{name}\"] = grad\n",
    "\n",
    "        embed_grads = self.embed.grads()\n",
    "        for name, grad in embed_grads.items():\n",
    "            grads[f\"embed.{name}\"] = grad\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    x = x - cp.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = cp.exp(x)\n",
    "    return exp_x / cp.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss_with_grad(logits, targets):\n",
    "    \"\"\"\n",
    "    logits: shape (B, T, V) — raw scores from final layer\n",
    "    targets: shape (B, T) — ground truth indices\n",
    "    \"\"\"\n",
    "    B, T, V = logits.shape\n",
    "    logits = logits.reshape(-1, V)  \n",
    "    targets = targets.reshape(-1)   \n",
    "    probs = softmax(logits) \n",
    "    loss = -cp.log(probs[np.arange(len(targets)), targets] + 1e-9)\n",
    "    avg_loss = cp.mean(loss)\n",
    "\n",
    "    dlogits = probs\n",
    "    dlogits[cp.arange(len(targets)), targets] -= 1\n",
    "    dlogits /= len(targets)\n",
    "\n",
    "    return avg_loss, dlogits.reshape(B, T, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a62a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Story.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd90143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(raw_text)\n",
    "\n",
    "if \"<sep>\" not in tokenizer.word2idx:\n",
    "    idx = len(tokenizer.word2idx)\n",
    "    tokenizer.word2idx[\"<sep>\"] = idx\n",
    "    tokenizer.idx2word[idx] = \"<sep>\"\n",
    "    tokenizer.vocab_size += 1\n",
    "\n",
    "encoded = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adadfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(encoded, seq_len):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(encoded) - seq_len):\n",
    "        inputs.append(encoded[i:i+seq_len])\n",
    "        targets.append(encoded[i+1:i+seq_len+1])\n",
    "    return cp.array(inputs), cp.array(targets)\n",
    "\n",
    "seq_len = 128\n",
    "X, Y = create_dataset(encoded, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c281352",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "hidden_dim = 64\n",
    "num_layers = 4\n",
    "max_len = seq_len\n",
    "model = DecoderOnlyTransformer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=max_len,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bd511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (1, 128, 669)\n",
      "Loss: 6.501418881921845\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.01)\n",
    "x_example = X[0]\n",
    "y_true = Y[0]\n",
    "logits, _ = model.forward(x_example, training=True)\n",
    "loss, dlogits = cross_entropy_loss_with_grad(logits, y_true)\n",
    "dx = model.backward_output_layer(dlogits)\n",
    "model.embed.backward(dx)\n",
    "grads = model.grads()\n",
    "optimizer.step(grads)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127df12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, tokenizer, start_text, length, k=5):\n",
    "    model_input = tokenizer.encode(start_text)\n",
    "    for _ in range(length):\n",
    "        x = cp.array(model_input[-seq_len:])\n",
    "        logits, _ = model.forward(x, training=False)\n",
    "        last_logits = logits[-1][:tokenizer.vocab_size].flatten()\n",
    "        top_k_indices = cp.argsort(last_logits)[-k:]               \n",
    "        top_k_logits = last_logits[top_k_indices]                 \n",
    "        top_k_probs = cp.exp(top_k_logits - cp.max(top_k_logits)) \n",
    "        top_k_probs /= cp.sum(top_k_probs)                    \n",
    "        top_k_indices = cp.asarray(top_k_indices).flatten()\n",
    "        top_k_probs = cp.asarray(top_k_probs).flatten()\n",
    "\n",
    "        assert top_k_indices.shape == top_k_probs.shape, f\"Shape mismatch: {top_k_indices.shape} vs {top_k_probs.shape}\"\n",
    "\n",
    "        # Sample next token\n",
    "        next_token = int(cp.random.choice(top_k_indices, size=1, p=top_k_probs)[0])\n",
    "        model_input.append(next_token)\n",
    "\n",
    "    return tokenizer.decode(model_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93a09ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy is using: b'NVIDIA GeForce RTX 3050 6GB Laptop GPU'\n"
     ]
    }
   ],
   "source": [
    "import cupy\n",
    "x = cupy.zeros((10000, 10000))\n",
    "print(\"CuPy is using:\", cupy.cuda.runtime.getDeviceProperties(0)['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 18.9796, Time: 104.77s\n",
      "Epoch 20, Loss: 14.9132, Time: 59.52s\n",
      "Epoch 40, Loss: 14.2524, Time: 60.01s\n",
      "Learning rate halved to 0.005000\n",
      "Epoch 60, Loss: 8.3190, Time: 186.94s\n",
      "Epoch 80, Loss: 8.1374, Time: 75.43s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def get_batches(X, Y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield cp.array(X[i:i+batch_size]), cp.array(Y[i:i+batch_size])\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for x_batch, y_batch in get_batches(X, Y, batch_size):\n",
    "        logits, cache = model.forward(x_batch) \n",
    "        loss, dlogits = cross_entropy_loss_with_grad(logits, y_batch) \n",
    "        model.backward(dlogits)  \n",
    "        for name, param in model.parameters():\n",
    "            optimizer.step(model.grads())\n",
    "\n",
    "        total_loss += loss\n",
    "        batch_count += 1\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    losses.append(avg_loss)\n",
    "    epoch_end = time.time()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Time: {epoch_end - epoch_start:.2f}s\")\n",
    "    if epoch != 0 and epoch % 50 == 0:\n",
    "        optimizer.lr *= 0.5\n",
    "        print(f\"Learning rate halved to {optimizer.lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "37441d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: is <unk> the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"is\"\n",
    "input_seq = prompt + \" <sep>\"\n",
    "output = sample(model, tokenizer, start_text=input_seq, length=15)\n",
    "print(\"Generated:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95e18816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 669\n",
      "Unique tokens in training data: 667\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Unique tokens in training data:\", len(set(tokenizer.encode(raw_text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4e65430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized: [613]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized:\", tokenizer.encode(\"universe\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
