{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def dropout(x, rate=0.1, training=True):\n",
    "    if not training or rate == 0:\n",
    "        return x\n",
    "    mask = (np.random.rand(*x.shape) > rate).astype(np.float32)\n",
    "    return x * mask / (1.0 - rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text):\n",
    "        cleaned = ''.join(c.lower() if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "        words = cleaned.split()\n",
    "\n",
    "        unique_words = sorted(list(set(words)))\n",
    "        unique_words.append(\"<unk>\")  \n",
    "\n",
    "        self.word2idx = {w: i for i, w in enumerate(unique_words)}\n",
    "        self.idx2word = {i: w for w, i in self.word2idx.items()}\n",
    "        self.vocab_size = len(unique_words)\n",
    "\n",
    "    def encode(self, text):\n",
    "        cleaned = ''.join(c.lower() if c.isalnum() or c.isspace() else ' ' for c in text)\n",
    "        words = cleaned.split()\n",
    "        return [self.word2idx.get(w, self.word2idx[\"<unk>\"]) for w in words]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ' '.join([self.idx2word[i] for i in indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "934be52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.weights = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "\n",
    "    def forward(self, indices):\n",
    "        return self.weights[indices] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f109a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding:\n",
    "    def __init__(self, max_len, embed_dim):\n",
    "        self.encoding = np.zeros((max_len, embed_dim))\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, embed_dim, 2):\n",
    "                self.encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i)/embed_dim)))\n",
    "                if i + 1 < embed_dim:\n",
    "                    self.encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i)/embed_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[0]\n",
    "        return x + self.encoding[:seq_len]\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)  \n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e90053e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention:\n",
    "    def __init__(self, embed_dim):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.scale = np.sqrt(embed_dim)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        scores = np.dot(Q, K.T) / self.scale \n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = np.where(mask == 0, -1e9, scores)  \n",
    "\n",
    "        weights = softmax(scores, axis=-1)  \n",
    "        output = np.dot(weights, V) \n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b93c44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.W_q = np.random.randn(embed_dim, embed_dim) * 0.01\n",
    "        self.W_k = np.random.randn(embed_dim, embed_dim) * 0.01\n",
    "        self.W_v = np.random.randn(embed_dim, embed_dim) * 0.01\n",
    "        self.W_o = np.random.randn(embed_dim, embed_dim) * 0.01\n",
    "\n",
    "        self.attn = ScaledDotProductAttention(self.head_dim)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        return x.reshape(x.shape[0], self.num_heads, self.head_dim)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        return x.reshape(x.shape[0], self.embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        Q = np.dot(x, self.W_q)\n",
    "        K = np.dot(x, self.W_k)\n",
    "        V = np.dot(x, self.W_v)\n",
    "\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "\n",
    "        heads_output = []\n",
    "        for i in range(self.num_heads):\n",
    "            out, _ = self.attn.forward(Q[:, i], K[:, i], V[:, i], mask)\n",
    "            heads_output.append(out)\n",
    "\n",
    "        concat = np.concatenate(heads_output, axis=-1)\n",
    "        return np.dot(concat, self.W_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daac79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        self.W1 = np.random.randn(embed_dim, hidden_dim) * 0.01\n",
    "        self.b1 = np.zeros((hidden_dim,))\n",
    "        self.W2 = np.random.randn(hidden_dim, embed_dim) * 0.01\n",
    "        self.b2 = np.zeros((embed_dim,))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(np.dot(x, self.W1) + self.b1)\n",
    "        return np.dot(x, self.W2) + self.b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3bc9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, embed_dim, eps=1e-5):\n",
    "        self.gamma = np.ones((embed_dim,))\n",
    "        self.beta = np.zeros((embed_dim,))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        var = ((x - mean) ** 2).mean(axis=-1, keepdims=True)\n",
    "        norm = (x - mean) / np.sqrt(var + self.eps)\n",
    "        return self.gamma * norm + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3a17f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock:\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout_rate=0.1):\n",
    "        self.ln1 = LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ln2 = LayerNorm(embed_dim)\n",
    "        self.ffn = FeedForward(embed_dim, hidden_dim)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x, mask, training=True):\n",
    "        attn_out = self.attn.forward(self.ln1.forward(x), mask)\n",
    "        attn_out = dropout(attn_out, rate=self.dropout_rate, training=training)\n",
    "        x = x + attn_out\n",
    "        ffn_out = self.ffn.forward(self.ln2.forward(x))\n",
    "        ffn_out = dropout(ffn_out, rate=self.dropout_rate, training=training)\n",
    "        x = x + ffn_out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063f0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer:\n",
    "    def __init__(self, vocab_size, max_len, embed_dim, num_heads, hidden_dim, num_layers):\n",
    "        self.embed = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(max_len, embed_dim)\n",
    "        self.blocks = [DecoderBlock(embed_dim, num_heads, hidden_dim, dropout_rate=0.1) for _ in range(num_layers)]\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "        self.output_layer = np.random.randn(embed_dim, vocab_size) * 0.01\n",
    "        \n",
    "    def forward(self, x_indices, training=True):\n",
    "        x = self.embed.forward(x_indices)\n",
    "        x = self.pos_enc.forward(x)\n",
    "        seq_len = x.shape[0]\n",
    "        mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x, mask, training=training)\n",
    "\n",
    "        x = self.ln.forward(x)\n",
    "        logits = np.dot(x, self.output_layer)  \n",
    "        return logits, x\n",
    "\n",
    "    def backward_output_layer(self, x, dlogits, lr):\n",
    "        dW_out = np.dot(x.T, dlogits)              \n",
    "        dx = np.dot(dlogits, self.output_layer.T)  \n",
    "        self.output_layer -= lr * dW_out\n",
    "        return dx  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss_with_grad(logits, targets):\n",
    "    probs = softmax(logits)\n",
    "    loss = -np.log(probs[np.arange(len(targets)), targets] + 1e-9)\n",
    "    avg_loss = np.mean(loss)\n",
    "    dlogits = probs.copy()\n",
    "    dlogits[np.arange(len(targets)), targets] -= 1\n",
    "    dlogits /= len(targets) \n",
    "\n",
    "    return avg_loss, dlogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b29e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Story.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfbdaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(raw_text)\n",
    "\n",
    "if \"<sep>\" not in tokenizer.word2idx:\n",
    "    idx = len(tokenizer.word2idx)\n",
    "    tokenizer.word2idx[\"<sep>\"] = idx\n",
    "    tokenizer.idx2word[idx] = \"<sep>\"\n",
    "    tokenizer.vocab_size += 1\n",
    "\n",
    "encoded = tokenizer.encode(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae838c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(encoded, seq_len):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(len(encoded) - seq_len):\n",
    "        inputs.append(encoded[i:i+seq_len])\n",
    "        targets.append(encoded[i+1:i+seq_len+1])\n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "49c308ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 128  \n",
    "X, Y = create_dataset(encoded, seq_len)\n",
    "embed_dim = 32\n",
    "num_heads = 2\n",
    "hidden_dim = 64\n",
    "num_layers = 4\n",
    "max_len = seq_len \n",
    "model = DecoderOnlyTransformer(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_len=max_len,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_layers=num_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2cf790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (128, 669)\n",
      "Loss: 6.500964440275334\n"
     ]
    }
   ],
   "source": [
    "x_example = X[0]  \n",
    "y_true = Y[0]    \n",
    "logits, x_out = model.forward(x_example, training=True)\n",
    "loss, dlogits = cross_entropy_loss_with_grad(logits, y_true)  \n",
    "dx = model.backward_output_layer(x_out, dlogits, lr=0.01)  \n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be6cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, tokenizer, start_text, length, k=5):\n",
    "    model_input = tokenizer.encode(start_text)\n",
    "    for _ in range(length):\n",
    "        x = np.array(model_input[-seq_len:])\n",
    "        logits, _ = model.forward(x, training=False)\n",
    "\n",
    "        last_logits = logits[-1]\n",
    "        \n",
    "        top_k_indices = np.argsort(last_logits)[-k:]  \n",
    "        top_k_probs = softmax(last_logits[top_k_indices])\n",
    "        next_token = np.random.choice(top_k_indices, p=top_k_probs)\n",
    "\n",
    "        model_input.append(next_token)\n",
    "    return tokenizer.decode(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ead7385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.7499\n",
      "Epoch 20, Loss: 5.5097\n",
      "Epoch 40, Loss: 5.5057\n",
      "Epoch 60, Loss: 5.5026\n",
      "Epoch 80, Loss: 5.4998\n",
      "Epoch 100, Loss: 5.4972\n",
      "Epoch 120, Loss: 5.4948\n",
      "Epoch 140, Loss: 5.4924\n",
      "Epoch 160, Loss: 5.4901\n",
      "Epoch 180, Loss: 5.4878\n",
      "Epoch 200, Loss: 5.4856\n",
      "Epoch 220, Loss: 5.4834\n",
      "Epoch 240, Loss: 5.4813\n",
      "Epoch 260, Loss: 5.4791\n",
      "Epoch 280, Loss: 5.4770\n",
      "Epoch 300, Loss: 5.4750\n",
      "Epoch 320, Loss: 5.4729\n",
      "Epoch 340, Loss: 5.4709\n",
      "Epoch 360, Loss: 5.4688\n",
      "Epoch 380, Loss: 5.4669\n",
      "Epoch 400, Loss: 5.4649\n",
      "Epoch 420, Loss: 5.4629\n",
      "Epoch 440, Loss: 5.4609\n",
      "Epoch 460, Loss: 5.4590\n",
      "Epoch 480, Loss: 5.4571\n"
     ]
    }
   ],
   "source": [
    "def get_batches(X, Y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield np.array(X[i:i+batch_size]), np.array(Y[i:i+batch_size])\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for x_batch, y_batch in get_batches(X, Y, batch_size):\n",
    "        for x_seq, y_seq in zip(x_batch, y_batch):\n",
    "            logits, x_out = model.forward(x_seq)\n",
    "            loss, dlogits = cross_entropy_loss_with_grad(logits, y_seq)\n",
    "            model.backward_output_layer(x_out, dlogits, lr=learning_rate)\n",
    "            total_loss += loss\n",
    "            batch_count += 1\n",
    "\n",
    "    avg_loss = total_loss / batch_count\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b96473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: beauty <unk> the the the is and is in in the the the the of in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"beauty\"\n",
    "input_seq = prompt + \" <sep>\"\n",
    "output = sample(model, tokenizer, start_text=input_seq, length=14)\n",
    "print(\"Generated:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
